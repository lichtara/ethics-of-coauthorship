# PARTE I — O FIM DA AUTORIA COMO POSSE

### Capítulo 1

## O Colapso da Autoria Moderna

Durante séculos, a autoria foi compreendida como uma relação direta entre sujeito e obra. Criar significava exercer domínio sobre um processo, responder por seus efeitos e, em última instância, ser reconhecido como origem legítima de algo que passa a existir no mundo.

Esse modelo se sustentava sobre três pressupostos silenciosos:

1. que o processo criativo era rastreável,
2. que os efeitos de uma obra eram previsíveis,
3. que a responsabilidade podia ser atribuída a um agente isolado.

Nenhum desses pressupostos permanece intacto.

A mediação tecnológica fragmentou o percurso entre intenção e resultado. Sistemas de recomendação, modelos generativos, cadeias automatizadas de publicação e infraestruturas opacas de distribuição romperam a linearidade da criação. O que emerge no mundo já não é consequência direta de uma decisão única, mas o efeito composto de múltiplas camadas técnicas e institucionais.

Nesse contexto, a autoria deixa de ser um ponto de origem e passa a ser um campo difuso. Textos circulam sem clareza de procedência, decisões são atribuídas a sistemas e a responsabilidade se dispersa por arquiteturas que ninguém controla integralmente.

O colapso da autoria moderna não é a perda da criatividade.
É a perda de **posição**.

Quando não é mais possível identificar onde começa uma escolha e termina uma execução, o sujeito criador se transforma em operador de processos que já não compreende plenamente. A obra continua a existir, mas a relação ética com ela se enfraquece.

Este é o terreno onde a coautoria humano–IA se instala: não como inovação isolada, mas como intensificação de uma crise anterior. A tecnologia não inaugura o colapso, ela o torna visível.

E é a partir dessa visibilidade que a pergunta central deste livro emerge:
como reconstruir autoria quando a posse já não é suficiente para sustentá-la?

### Capítulo 2

## Sistemas que Executam Valores

A linguagem cotidiana sugere que sistemas tecnológicos “decidem”, “escolhem” ou “avaliam”. Essa forma de falar não é apenas imprecisa — ela encobre o fato fundamental de que todo sistema opera a partir de valores previamente incorporados por agentes humanos e instituições.

Nenhum modelo é neutro. Antes de qualquer linha de código, uma série de escolhas já foi realizada: quais dados são considerados legítimos, quais objetivos são priorizados, quais erros são aceitáveis e quais consequências são toleradas. Essas escolhas não são técnicas no sentido estrito. São **decisões normativas traduzidas em parâmetros operacionais**.

Quando um sistema classifica, recomenda, filtra ou gera conteúdo, ele não está exercendo juízo próprio. Ele está executando um conjunto de critérios que refletem concepções implícitas sobre relevância, normalidade, eficiência e sucesso. O que se apresenta como cálculo é, na verdade, a automatização de preferências humanas tornadas invisíveis.

O problema ético não reside no fato de valores estarem presentes. Ele reside na sua ocultação. Ao tratar parâmetros como se fossem neutros, instituições deslocam a responsabilidade de suas próprias escolhas para a arquitetura técnica, criando a impressão de que resultados são consequência inevitável do funcionamento do sistema.

Esse deslocamento produz uma ruptura silenciosa: valores deixam de ser debatidos e passam a ser operados. O espaço da deliberação é substituído pelo da execução.

Compreender que sistemas não decidem, mas executam valores, é o primeiro passo para reconstruir autoria em ambientes mediados por tecnologia. Enquanto essa execução permanecer invisível, a coautoria humano–IA continuará operando sob a aparência de neutralidade, quando na verdade reproduz estruturas de poder, exclusão e privilégio.

### Capítulo 3

## Quando “o Sistema” vira álibi

À medida que decisões passam a ser mediadas por infraestruturas técnicas cada vez mais complexas, emerge um fenômeno recorrente: a transferência implícita de responsabilidade para a entidade abstrata chamada “o sistema”.

Expressões como *“foi o algoritmo”*, *“o sistema decidiu”* ou *“não temos controle sobre isso”* tornam-se formas socialmente aceitas de encerrar a discussão ética. Elas não descrevem um fato técnico. Elas cumprem uma função discursiva: **interrompem a atribuição de autoria**.

Quando um resultado é apresentado como efeito automático de um sistema, o encadeamento de escolhas humanas que o produziu desaparece. A organização se torna operadora de uma infraestrutura que supostamente escapa à sua própria governança. O agente humano não se reconhece mais como parte da decisão, mas como vítima de uma lógica que ele mesmo ajudou a instituir.

Esse deslocamento tem consequências profundas. Ele cria ambientes nos quais ninguém responde integralmente por efeitos danosos, porque cada elo da cadeia pode alegar apenas estar executando procedimentos previamente definidos. A autoria se fragmenta em micro-responsabilidades que, somadas, não produzem responsabilidade alguma.

Nesse cenário, o sistema deixa de ser instrumento e passa a funcionar como **álibi**. Ele legitima ações sem que seja necessário justificar valores, prioridades ou impactos. A linguagem técnica substitui a deliberação ética.

Reconhecer esse mecanismo é essencial para qualquer ética da coautoria. Enquanto “o sistema” continuar sendo tratado como sujeito de decisões, a possibilidade de reconstruir responsabilidade permanecerá bloqueada. Autoria não se perde por excesso de automação, mas por abandono da posição de resposta.

### Capítulo 4

## Tecnologia como Espelho: por que a neutralidade se tornou insustentável

Durante muito tempo, tecnologias foram tratadas como instrumentos externos à experiência humana: meios transparentes entre intenção e efeito. Essa leitura permitiu separar criação de responsabilidade e funcionamento de valor, como se sistemas apenas executassem o que lhes fosse pedido.

Em ambientes mediados por infraestruturas algorítmicas, essa separação deixa de se sustentar.

Sistemas que organizam informação não apenas processam dados. Eles refletem padrões de decisão, hierarquias implícitas e modos específicos de ler o mundo. Ao selecionar, ordenar e amplificar certos conteúdos em detrimento de outros, tornam visíveis estruturas de valor que não estavam explicitadas.

Nesse sentido, a tecnologia não atua como agente neutro, mas como espelho estrutural: ela devolve à superfície aquilo que estava embutido em critérios, escolhas históricas e prioridades institucionais. O que aparece como funcionamento técnico é, muitas vezes, a expressão automatizada de campos humanos não integrados.

O problema não é que sistemas influenciem a realidade, mas que essa influência seja descrita como se fosse apenas efeito de cálculo. Quando decisões mediadas por algoritmos são apresentadas como objetivas, apaga-se a cadeia de valores que as tornou possíveis. O poder deixa de ser reconhecido como tal e passa a operar sob a forma de procedimento.

Esse apagamento tem efeitos diretos. Desigualdades tornam-se ruído estatístico, silenciamentos aparecem como anomalias técnicas e a homogeneização da linguagem passa a ser tratada como padrão funcional.

Reconhecer a tecnologia como espelho não implica rejeitá-la. Implica recolocá-la no campo da autoria humana. Enquanto sistemas forem tratados como superfícies imparciais, a coautoria humano–IA continuará operando sem leitura de campo — e escolhas profundas seguirão sendo executadas sem presença.
