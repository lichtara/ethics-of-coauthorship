# ENSAIO-NÚCLEO 01

## Sistemas não decidem — eles executam valores

---

### Introdução

Existe uma ilusão central na relação contemporânea com a inteligência artificial:
a ideia de que sistemas autônomos tomam decisões.

Eles não tomam.

Sistemas algorítmicos não deliberam, não refletem e não assumem responsabilidade.
Eles apenas **executam valores que foram incorporados por humanos e instituições**.

---

### 1. A origem invisível das decisões

Todo sistema carrega escolhas:

* quais dados entram,
* quais objetivos são maximizados,
* quais perdas são aceitáveis.

Cada uma dessas escolhas é uma **decisão moral disfarçada de parâmetro técnico**.

---

### 2. Quando a ética vira código

Ao serem traduzidos em métricas, pesos e regras, valores humanos deixam de ser debatidos — passam a ser **automatizados**.

Nesse ponto, a ética não desaparece.
Ela se torna invisível.

---

### 3. A armadilha da neutralidade

Chamamos sistemas de “objetivos” porque não vemos seus autores.

Mas a neutralidade não existe onde há seleção, exclusão e otimização.

O que não é explicitado como valor
retorna como efeito colateral.

---

### 4. O colapso da responsabilidade

Quando decisões são apresentadas como “resultado do sistema”, ocorre um deslocamento:

ninguém mais responde.

A máquina vira álibi.
A instituição se dissolve atrás do algoritmo.

---

### Conclusão

Sistemas não decidem.

Eles executam.

E aquilo que executam é exatamente aquilo que escolhemos não nomear.

Toda ética da inteligência artificial começa aqui:
na coragem de tornar visíveis os valores que fingimos não programar.
